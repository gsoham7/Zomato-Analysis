{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "Ou-I18pAyIpj",
        "hwyV_J3ipUZe",
        "dEUvejAfpUZe",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "578E2V7j08f6",
        "GMQiZwjn3iu7",
        "cJNqERVU536h",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Zomato Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Summary – Zomato Restaurant Reviews Analysis\n",
        "The “Zomato Analysis” project focuses on understanding customer preferences, restaurant performance, and business insights using data provided by Zomato. The aim was to extract valuable insights from customer reviews and restaurant metadata using Exploratory Data Analysis (EDA), Natural Language Processing (NLP), and Machine Learning (ML) techniques. This end-to-end project not only highlights patterns in customer behavior but also builds a predictive model to classify review sentiments, which can aid Zomato in improving services and customer engagement.\n",
        "\n",
        "The project started by combining two datasets: one containing restaurant metadata (names, cuisines, cost, timings, etc.) and the other featuring customer reviews and ratings. After importing the data, a thorough data inspection was performed. This included checking for missing values, duplicate entries, data types, and null entries. Missing values in crucial columns were either imputed or dropped based on relevance, and data cleaning included converting cost and rating columns into appropriate numeric formats for analysis.\n",
        "\n",
        "To enable meaningful visual exploration, the cleaned data was subjected to various univariate and bivariate visualizations. These included the distribution of ratings, the relationship between cost and rating, average ratings across different cuisines, and a correlation heatmap. These visuals helped identify trends such as higher-rated cuisines, the pricing sweet spot for restaurants, and which metadata features (like number of reviews or followers) correlated with better ratings.\n",
        "\n",
        "For hypothesis testing, three assumptions were tested:\n",
        "\n",
        "Higher-cost restaurants tend to receive better ratings.\n",
        "\n",
        "Restaurants with more reviews are rated more favorably.\n",
        "\n",
        "Reviewers with more followers give higher ratings.\n",
        "\n",
        "Statistical testing (Pearson correlation) showed that while these factors had some influence, none of them had a very strong positive correlation with rating. This indicates that customer sentiment may depend on other qualitative factors such as service, taste, ambiance, or expectations.\n",
        "\n",
        "The next phase focused on Natural Language Processing. Text reviews were preprocessed using standard NLP techniques including lowercasing, punctuation removal, URL removal, stopword elimination, lemmatization, and tokenization. The cleaned reviews were vectorized using the TF-IDF (Term Frequency-Inverse Document Frequency) technique to transform text data into numerical format suitable for ML models.\n",
        "\n",
        "Three ML models were trained to classify sentiment:\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Support Vector Machine (SVM)\n",
        "\n",
        "Random Forest Classifier\n",
        "\n",
        "Among these, the Random Forest Classifier performed the best in terms of F1-score, which was chosen as the main evaluation metric due to the balanced need for precision and recall in classifying sentiments accurately. Hyperparameter tuning using GridSearchCV further improved the model performance.\n",
        "\n",
        "Feature importance from the final model revealed that the number of reviews, cost, and certain keywords from reviews played a significant role in predicting sentiment. These insights are valuable for Zomato in multiple ways—ranging from recommending top restaurants, improving customer targeting, to identifying areas where restaurants may need improvement.\n",
        "\n",
        "In conclusion, the project demonstrates how a combination of data analysis, NLP, and machine learning can deliver actionable business intelligence. It enables customers to find better restaurants and allows Zomato to make data-driven decisions to improve their platform and restaurant partnerships."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato, being one of the largest food delivery and restaurant discovery platforms, gathers massive volumes of user-generated data through reviews, ratings, and restaurant metadata. However, this valuable data often remains underutilized.\n",
        "\n",
        "The goal of this project is to extract actionable insights from Zomato’s restaurant metadata and customer reviews through exploratory data analysis, sentiment analysis, and machine learning. This will help:\n",
        "\n",
        "Customers discover top-rated restaurants in their locality.\n",
        "\n",
        "Zomato/Restaurant owners understand the factors affecting customer satisfaction and identify areas for business improvement.\n",
        "\n",
        "Cluster restaurants based on cuisine, cost, and ratings for strategic segmentation and targeting."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from scipy import stats\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = pd.read_csv(\"/Zomato_Restaurant_names_and_Metadata.csv\")\n",
        "reviews_df = pd.read_csv(\"/Zomato_Restaurant_reviews.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metadata_df.head())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Metadata Shape:\", metadata_df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(metadata_df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Metadata Duplicates:\", metadata_df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(metadata_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata has missing values in 'Collections' and 'Timings'. Reviews have a few missing entries in Reviewer, Review, Rating, Metadata and Time."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(metadata_df.columns)\n",
        "print(reviews_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(metadata_df.describe(include='all'))\n",
        "\n",
        "print(reviews_df.describe(include='all'))\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displayed column names, described datasets, and counted unique values.\n",
        "\n",
        "This helped in identifying key categorical and numerical features, e.g., cuisines, cost, ratings."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in metadata_df.columns:\n",
        "    print(f\"{col}: {metadata_df[col].nunique()} unique values\")\n",
        "    print()\n",
        "\n",
        "for col in reviews_df.columns:\n",
        "    print(f\"{col}: {reviews_df[col].nunique()} unique values\")\n",
        ""
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean 'Cost' column in metadata_df\n",
        "metadata_df['Cost'] = metadata_df['Cost'].replace(\",\", \"\", regex=True).astype(float)\n",
        "\n",
        "# Clean 'Rating' column in reviews_df\n",
        "reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing reviews or ratings\n",
        "reviews_df.dropna(subset=['Review', 'Rating'], inplace=True)\n",
        "\n",
        "# Fill missing Reviewers with 'Anonymous'\n",
        "reviews_df['Reviewer'] = reviews_df['Reviewer'].fillna('Anonymous')\n",
        "\n",
        "\n",
        "# Split Metadata to extract numeric values\n",
        "reviews_df[['Num_Reviews', 'Num_Followers']] = reviews_df['Metadata'].str.extract(r'(\\d+) Review.*?(\\d+) Follower')\n",
        "reviews_df['Num_Reviews'] = pd.to_numeric(reviews_df['Num_Reviews'], errors='coerce')\n",
        "reviews_df['Num_Followers'] = pd.to_numeric(reviews_df['Num_Followers'], errors='coerce')\n",
        "\n",
        "# Merge datasets on restaurant name\n",
        "combined_df = reviews_df.merge(metadata_df, left_on='Restaurant', right_on='Name', how='left')\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaned Cost and Rating for numeric analysis.\n",
        "\n",
        "Extracted number of reviews/followers from metadata.\n",
        "\n",
        "Merged datasets for combined analysis.\n",
        "\n",
        "Insight: Some high-cost restaurants don’t always align with high ratings."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Chart 1 - Distribution of Ratings\n",
        "sns.histplot(combined_df['Rating'], bins=10, kde=True)\n",
        "plt.title(\"Distribution of Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how customer ratings are spread (are they mostly positive, neutral, or negative?)."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals if the majority of ratings are high (positive sentiment) or skewed towards low ratings."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies overall satisfaction levels, highlighting if quality improvement is needed."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Chart 2 - Cost vs Rating\n",
        "sns.scatterplot(data=combined_df, x='Cost', y='Rating')\n",
        "plt.title(\"Cost vs Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze if higher-cost restaurants tend to have higher ratings."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows whether expensive restaurants truly deliver better experiences or if affordable ones perform equally well."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps restaurants plan pricing strategies without harming customer perception."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Chart 3 - Average Rating per Cuisine\n",
        "cuisine_ratings = combined_df.groupby('Cuisines')['Rating'].mean().sort_values(ascending=False).head(10)\n",
        "cuisine_ratings.plot(kind='barh')\n",
        "plt.title(\"Top 10 Cuisines by Average Rating\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify which cuisines receive the highest customer satisfaction."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the top 10 cuisines by average rating, indicating what customers like most."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guides restaurants on trending cuisines, helping new ventures choose the best cuisine to serve."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Load your datasets\n",
        "metadata_df = pd.read_csv(\"/Zomato_Restaurant_names_and_Metadata.csv\")\n",
        "reviews_df = pd.read_csv(\"/Zomato_Restaurant_reviews.csv\")\n",
        "\n",
        "# Clean Cost column\n",
        "metadata_df['Cost'] = metadata_df['Cost'].replace('[^0-9]', '', regex=True).astype(float)\n",
        "\n",
        "# Merge datasets for richer analysis\n",
        "merged_df = reviews_df.merge(metadata_df, how='left', left_on='Restaurant', right_on='Name')\n"
      ],
      "metadata": {
        "id": "MFAUqjzsywWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "top_reviewed = reviews_df['Restaurant'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_reviewed.values, y=top_reviewed.index, palette='Blues_r')\n",
        "plt.title('Top 10 Most Reviewed Restaurants')\n",
        "plt.xlabel('Number of Reviews')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify which restaurants generate the highest customer engagement."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the most popular restaurants based on review count."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Helps prioritize partnerships, promotions, and marketing for high-engagement restaurants."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Force 'Rating' to be numeric, convert errors to NaN\n",
        "reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid/missing ratings (NaNs)\n",
        "reviews_df = reviews_df.dropna(subset=['Rating'])\n"
      ],
      "metadata": {
        "id": "5qpRMR0YzXgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "avg_ratings = reviews_df.groupby('Restaurant')['Rating'].mean().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=avg_ratings.values, y=avg_ratings.index, palette='Greens_r')\n",
        "plt.title('Top 10 Restaurants by Average Rating')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To highlight restaurants with the best customer satisfaction."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals top-rated restaurants, even if they have fewer reviews."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be promoted as premium/high-quality options to attract more customers."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(metadata_df['Cost'], bins=20, kde=True, color='orange')\n",
        "plt.title('Distribution of Restaurant Cost for Two')\n",
        "plt.xlabel('Cost for Two')\n",
        "plt.ylabel('Number of Restaurants')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the pricing landscape of restaurants."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the common cost ranges (e.g., affordable vs. premium)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in pricing strategies and targeting the right customer segment."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "cuisine_series = metadata_df['Cuisines'].dropna().str.split(', ')\n",
        "all_cuisines = Counter([c for sublist in cuisine_series for c in sublist])\n",
        "common_cuisines = pd.Series(dict(all_cuisines)).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=common_cuisines.values, y=common_cuisines.index, palette='coolwarm')\n",
        "plt.title('Top 10 Most Common Cuisines')\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Cuisine')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see which cuisines dominate the market."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals customer preferences for certain cuisines like North Indian, Chinese, etc."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guides new restaurant owners or aggregators on what cuisine is in demand."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(reviews_df['Pictures'], bins=10, color='purple')\n",
        "plt.title('Number of Pictures Shared per Review')\n",
        "plt.xlabel('Pictures per Review')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure how visually engaging restaurants are."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some restaurants drive more photo-sharing, indicating better presentation/ambience."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encourages restaurants to improve plating & ambience to drive social media exposure."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x='Rating', data=reviews_df, palette='magma')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the overall sentiment of customers."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows whether ratings skew positive, neutral, or negative."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies if customer satisfaction is generally high or if improvements are needed."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Rating column\n",
        "merged_df['Rating'] = pd.to_numeric(merged_df['Rating'], errors='coerce')\n",
        "\n",
        "# Clean Cost column\n",
        "merged_df['Cost'] = merged_df['Cost'].replace(',', '', regex=True)\n",
        "merged_df['Cost'] = pd.to_numeric(merged_df['Cost'], errors='coerce')\n",
        "\n",
        "# Drop rows where Cost or Rating is missing or invalid\n",
        "merged_df = merged_df.dropna(subset=['Cost', 'Rating'])\n",
        "\n"
      ],
      "metadata": {
        "id": "xh1hxrndz1zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Now group and plot\n",
        "avg_rating_cost = merged_df.groupby('Restaurant')[['Cost', 'Rating']].mean()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(data=avg_rating_cost, x='Cost', y='Rating', hue='Rating', palette='viridis')\n",
        "plt.title('Cost vs Average Rating')\n",
        "plt.xlabel('Average Cost for Two')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "# Optional: remove extreme cost outliers\n",
        "Q1 = avg_rating_cost['Cost'].quantile(0.25)\n",
        "Q3 = avg_rating_cost['Cost'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "avg_rating_cost = avg_rating_cost[(avg_rating_cost['Cost'] >= Q1 - 1.5 * IQR) & (avg_rating_cost['Cost'] <= Q3 + 1.5 * IQR)]\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if higher cost means better ratings."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reveals correlation (if any) between pricing and customer satisfaction."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in pricing strategy—whether premium pricing aligns with higher perceived value.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "top_pictures = reviews_df.groupby('Restaurant')['Pictures'].sum().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=top_pictures.values, y=top_pictures.index, palette='Purples_r')\n",
        "plt.title('Top 10 Restaurants by Total Review Pictures')\n",
        "plt.xlabel('Total Pictures')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see which restaurants create the most visual buzz."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High picture counts suggest strong visual appeal/experience."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be used in visual marketing campaigns and social media promotions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "collection_series = metadata_df['Collections'].dropna().str.split(', ')\n",
        "all_collections = Counter([c for sublist in collection_series for c in sublist])\n",
        "top_collections = pd.Series(dict(all_collections)).sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_collections.values, y=top_collections.index, palette='Set2')\n",
        "plt.title('Top 10 Food Collections')\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Collection')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see popular thematic collections (e.g., best bars, late-night spots)."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows which collections attract the most restaurants."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps design targeted campaigns (e.g., “Late-night delivery week”).\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "reviews_df['Time'] = pd.to_datetime(reviews_df['Time'], errors='coerce')\n",
        "daily_reviews = reviews_df['Time'].dt.date.value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(daily_reviews.index, daily_reviews.values, color='teal')\n",
        "plt.title('Daily Review Activity')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how review trends change over time."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows spikes in review activity, possibly linked to events or promotions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps plan marketing campaigns around peak engagement periods."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "corr = combined_df[['Rating', 'Cost', 'Num_Reviews', 'Num_Followers']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly identify relationships between key numeric variables like Rating, Cost, Number of Reviews, and Followers."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highlights which variables are positively or negatively correlated (e.g., more reviews might correlate with higher ratings, or cost might have little correlation).\n",
        "Helps focus on impactful factors (e.g., if followers drive ratings, encourage follower engagement strategies)."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(combined_df[['Rating', 'Cost', 'Num_Reviews', 'Num_Followers']].dropna())\n",
        "plt.suptitle(\"Pair Plot\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize distributions and pairwise relationships among multiple variables at once."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows patterns, clusters, and possible linear/non-linear trends between variables (e.g., cost vs followers).\n",
        "Helps detect multi-variable trends that can guide restaurant pricing, marketing, or review engagement strategies."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Hypothesis 1: Higher cost restaurants have higher ratings.\n",
        "# H0: Cost and Rating are independent (no correlation)\n",
        "# H1: Cost and Rating are positively correlated\n",
        "corr_test_1 = stats.pearsonr(combined_df['Cost'].dropna(), combined_df['Rating'].dropna())\n",
        "print(\"Cost vs Rating Correlation Test:\", corr_test_1)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost vs Rating Correlation Test: PearsonRResult(statistic=0.1441, pvalue=2.419e-47)\n",
        "\n",
        "Hypothesis Recap:\n",
        "Null Hypothesis (H₀): Cost and Rating are independent (no correlation).\n",
        "\n",
        "Alternative Hypothesis (H₁): Cost and Rating are positively correlated.\n",
        "\n",
        "Interpretation:\n",
        "Pearson correlation coefficient (r): 0.1441\n",
        "\n",
        "This indicates a weak positive correlation between Cost and Rating.\n",
        "\n",
        "P-value: 2.419e-47\n",
        "\n",
        "This is extremely small, much smaller than any common significance level (e.g., 0.05, 0.01).\n",
        "\n",
        "Conclusion:\n",
        "Since the p-value is significantly less than 0.05, you reject the null hypothesis (H₀).\n",
        "\n",
        "Final Decision:\n",
        "We accept the alternative hypothesis (H₁): There is a statistically significant (though weak) positive correlation between cost and rating of restaurants.\n",
        "\n",
        "Business Insight:\n",
        "More expensive restaurants tend to receive slightly higher ratings — which could imply better service, quality, or experience. While the correlation is weak, it's statistically significant and worth considering for pricing strategy and market segmentation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis 2: Restaurants with more reviews have higher average ratings.\n",
        "# H0: Number of reviews and rating are independent\n",
        "# H1: More reviews correlate with higher rating\n",
        "# Drop rows where either Num_Reviews or Rating is missing\n",
        "subset_df = combined_df[['Num_Reviews', 'Rating']].dropna()\n",
        "\n",
        "# Now run Pearson correlation\n",
        "corr_test_2 = stats.pearsonr(subset_df['Num_Reviews'], subset_df['Rating'])\n",
        "print(\"Num Reviews vs Rating Correlation Test:\", corr_test_2)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "Pearson correlation coefficient (r): 0.0367\n",
        "→ Very weak positive correlation (almost negligible)\n",
        "\n",
        "P-value: 0.00078\n",
        "→ Statistically significant, since it's well below the 0.05 threshold\n",
        "\n",
        "Conclusion:\n",
        "Although the correlation is statistically significant (because of the very low p-value), the correlation strength is extremely weak.\n",
        "\n",
        "You still reject the null hypothesis (H₀), but with a strong caveat.\n",
        "\n",
        "Final Decision:\n",
        "Reject H₀ and accept H₁, but acknowledge:\n",
        "There is statistical evidence of a relationship between the number of reviews and ratings, but the practical significance is negligible.\n",
        "\n",
        "Business Insight:\n",
        "The number of reviews does not meaningfully influence the average rating.\n",
        "Popular restaurants (with many reviews) do not necessarily have better ratings.\n",
        "Focusing on quality rather than sheer number of reviews is likely more important."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Hypothesis 3: Reviewers with more followers give higher ratings.\n",
        "# H0: Followers count has no effect on rating\n",
        "# H1: More followers correlate with higher ratings\n",
        "# corr_test_3 = stats.pearsonr(combined_df['Num_Followers'].dropna(), combined_df['Rating'].dropna())\n",
        "# print(\"Num Followers vs Rating Correlation Test:\", corr_test_3)\n",
        "\n",
        "# Drop rows where either Num_Followers or Rating is missing\n",
        "subset_df = combined_df[['Num_Followers', 'Rating']].dropna()\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr_test_3 = stats.pearsonr(subset_df['Num_Followers'], subset_df['Rating'])\n",
        "print(\"Num Followers vs Rating Correlation Test:\", corr_test_3)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "Pearson Correlation Coefficient (r): 0.0375\n",
        "→ Indicates a very weak positive correlation, close to zero.\n",
        "\n",
        "P-value: 0.0006\n",
        "→ This is statistically significant, since it’s less than 0.05.\n",
        "\n",
        "Conclusion:\n",
        "The correlation is statistically significant but extremely weak in magnitude.\n",
        "\n",
        "Therefore, we reject the null hypothesis (H₀), but we must interpret this carefully.\n",
        "\n",
        "Final Decision:\n",
        "Reject H₀ in favor of H₁ — there is a statistically significant correlation between number of followers and rating.\n",
        "However, the impact is too weak to be practically useful in decision-making.\n",
        "\n",
        "Business Insight:\n",
        "While influential reviewers (more followers) slightly tend to give higher ratings, the difference is negligible.\n",
        "\n",
        "This suggests that reviewer credibility/followership does not meaningfully affect how they rate restaurants.\n",
        "\n",
        "It may not be beneficial to overly prioritize highly-followed reviewers in sentiment modeling or ranking systems."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe and future-proof missing value imputation\n",
        "combined_df['Collections'] = combined_df['Collections'].fillna('None')\n",
        "combined_df['Timings'] = combined_df['Timings'].fillna('Unknown')\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Outlier Handling - Cost\n",
        "Q1 = combined_df['Cost'].quantile(0.25)\n",
        "Q3 = combined_df['Cost'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "filtered_df = combined_df[(combined_df['Cost'] >= Q1 - 1.5 * IQR) & (combined_df['Cost'] <= Q3 + 1.5 * IQR)]"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Categorical Encoding - Cuisine\n",
        "# label_encoder = LabelEncoder()\n",
        "# filtered_df['Cuisine_Label'] = label_encoder.fit_transform(filtered_df['Cuisines'])\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_df.loc[:, 'Cuisine_Label'] = label_encoder.fit_transform(filtered_df['Cuisines'])\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this preprocessing step, we focused on cleaning, handling outliers, and preparing categorical data for machine learning:\n",
        "\n",
        "**Missing Value Imputation**\n",
        "\n",
        "The Collections column had missing values which were safely filled with 'None'.\n",
        "\n",
        "The Timings column had missing values filled with 'Unknown'.\n",
        "This ensures no null values disrupt the analysis or model training.\n",
        "\n",
        "**Outlier Treatment (Cost Column)**\n",
        "\n",
        "We used the Interquartile Range (IQR) method to detect and remove outliers from the Cost column.\n",
        "\n",
        "Rows with cost values outside 1.5 * IQR from Q1 and Q3 were excluded.\n",
        "This helps reduce skewness and improves model robustness by focusing only on typical spending patterns.\n",
        "\n",
        "**Categorical Encoding**\n",
        "\n",
        "The Cuisines column (a categorical text feature) was converted into numeric form using Label Encoding.\n",
        "\n",
        "This step transforms cuisine types into machine-readable labels without creating high-dimensional data.\n",
        "Makes the data ready for model input while preserving category distinctions.\n",
        "\n",
        "Together, these steps ensure the dataset is clean, balanced, and ready for high-quality machine learning model development."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "combined_df['Clean_Review'] = combined_df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: re.sub(r'http\\S+|www\\S+|\\w*\\d\\w*', '', x))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "combined_df['Clean_Review'] = combined_df['Review'].str.lower()\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: re.sub(r'http\\S+|www\\S+|\\w*\\d\\w*', '', x))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "combined_df['Clean_Review'] = combined_df['Clean_Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "combined_df['Tokens'] = combined_df['Clean_Review'].apply(nltk.word_tokenize)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "combined_df['Lemmatized'] = combined_df['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "combined_df['POS_Tags'] = combined_df['Lemmatized'].apply(nltk.pos_tag)\n",
        "combined_df['Processed_Text'] = combined_df['Lemmatized'].apply(lambda x: ' '.join(x))\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = vectorizer.fit_transform(combined_df['Processed_Text'])\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "combined_df['Processed_Text'] = combined_df['Lemmatized'].apply(lambda x: ' '.join(x))\n",
        "tfidf_matrix = vectorizer.fit_transform(combined_df['Processed_Text'])"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Safe missing value imputation — avoid inplace=True\n",
        "combined_df['Collections'] = combined_df['Collections'].fillna('None')\n",
        "combined_df['Timings'] = combined_df['Timings'].fillna('Unknown')\n",
        "\n",
        "# Outlier removal using IQR\n",
        "Q1 = combined_df['Cost'].quantile(0.25)\n",
        "Q3 = combined_df['Cost'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "filtered_df = combined_df[(combined_df['Cost'] >= Q1 - 1.5 * IQR) &\n",
        "                          (combined_df['Cost'] <= Q3 + 1.5 * IQR)].copy()  # <- use .copy() to avoid chained assignment issues\n",
        "\n",
        "# Label Encoding (safe way)\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_df.loc[:, 'Cuisine_Label'] = label_encoder.fit_transform(filtered_df['Cuisines'])\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Data normalization or transformation may help algorithms sensitive to data distribution.\n",
        "# Here, we normalize numerical features like Cost, Num_Reviews, Num_Followers.\n",
        "scaler = StandardScaler()\n",
        "filtered_df[['Cost_Scaled', 'Num_Reviews_Scaled', 'Num_Followers_Scaled']] = scaler.fit_transform(\n",
        "    filtered_df[['Cost', 'Num_Reviews', 'Num_Followers']])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "# 9. Dimensionality Reduction\n",
        "# Since TF-IDF matrix is high-dimensional, apply PCA to reduce to fewer dimensions for clustering.\n",
        "pca = PCA(n_components=50)\n",
        "tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# it transforms data into new set of variables called principal components.\n",
        "#the first component captures the most variance and the subsequent captures decreasing amount of variance"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# 10. Data Splitting\n",
        "# Use 80-20 train-test split for generalization.\n",
        "# X_train, X_test, y_train, y_test = train_test_split(tfidf_reduced, filtered_df['Rating'], test_size=0.2, random_state=42)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. TF-IDF on filtered data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(filtered_df['Clean_Review'])\n",
        "\n",
        "# Optional: Dimensionality reduction\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# 2. Train-test split (Now dimensions match)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_reduced, filtered_df['Rating'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vShM_VuQnNLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Check rating distribution\n",
        "# print(filtered_df['Rating'].value_counts())\n",
        "# # If imbalanced (e.g., skewed to high ratings), use SMOTE\n",
        "# smote = SMOTE()\n",
        "# X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "y_train_class = y_train.astype(str)  # e.g., '4.5', '3.0'\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train_class)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use SMOTE (Synthetic Minority Oversampling Technique) to handle imbalanced datasets in machine learning when one class (typically the one we care about most, like fraud detection or negative reviews) is significantly underrepresented compared to the majority class.\n",
        "\n",
        "⚠️ Why Imbalanced Data Is a Problem:\n",
        "If your dataset is imbalanced (e.g., 95% positive and 5% negative ratings), most ML models will:\n",
        "\n",
        "Bias toward the majority class (e.g., always predict “positive”)\n",
        "\n",
        "Have misleading accuracy — you might get 95% accuracy just by guessing the majority every time\n",
        "\n",
        "Perform poorly on the class you actually care about (e.g., identifying dissatisfied customers)\n",
        "\n",
        "✅ Why Use SMOTE:\n",
        "SMOTE tackles this by:\n",
        "\n",
        "Synthesizing new data points for the minority class (instead of duplicating existing ones).\n",
        "\n",
        "Creating synthetic examples by interpolating between existing minority class instances and their nearest neighbors.\n",
        "\n",
        "Balancing the dataset to give the model a fair chance to learn both classes properly.\n",
        "\n",
        "📊 Example:\n",
        "Class\tCount Before\tCount After SMOTE\n",
        "Positive Reviews\t950\t950\n",
        "Negative Reviews\t50\t950\n",
        "\n",
        "🧠 Benefits of SMOTE:\n",
        "Prevents overfitting (unlike random oversampling).\n",
        "\n",
        "Improves recall, F1-score, and AUC.\n",
        "\n",
        "Helps the model learn patterns of the minority class more effectively.\n",
        "\n",
        "🛑 When Not to Use:\n",
        "When your data is very noisy — SMOTE might amplify noise.\n",
        "\n",
        "When your dataset is very large — can increase training time.\n",
        "\n",
        "📌 Summary:\n",
        "SMOTE is used to balance the class distribution in a dataset by creating synthetic minority class examples, leading to better generalization, fairer predictions, and improved performance metrics for models trained on imbalanced data"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create Sentiment Labels\n",
        "filtered_df['Sentiment'] = filtered_df['Rating'].apply(lambda x: 1 if x >= 3.5 else 0)\n",
        "\n",
        "# 2. TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(filtered_df['Clean_Review'])\n",
        "tfidf_features = tfidf_matrix.toarray()\n",
        "\n",
        "# 3. Select and scale numeric features\n",
        "numeric_features = filtered_df[['Cost', 'Num_Reviews', 'Num_Followers']].fillna(0)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# 4. Combine features\n",
        "X = np.concatenate((tfidf_features, X_scaled), axis=1)\n",
        "y = filtered_df['Sentiment']\n",
        "\n",
        "# 5. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 6. Apply SMOTE to handle imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "M6kvx1y2ovU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# ========== ML Model 1: Random Forest Classifier ==========\n",
        "\n",
        "# Fit the algorithm\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📘 Key Metrics Explained:\n",
        "Precision: Out of all the predictions the model made for a class, how many were correct?\n",
        "\n",
        "Class 0: 82% of negative predictions were actually negative.\n",
        "\n",
        "Class 1: 88% of positive predictions were actually positive.\n",
        "\n",
        "Recall: Out of all actual instances of a class, how many did the model correctly identify?\n",
        "\n",
        "Class 0: 79% of true negatives were caught.\n",
        "\n",
        "Class 1: 89% of true positives were caught.\n",
        "\n",
        "F1-Score: Harmonic mean of precision and recall. A good balance indicator.\n",
        "\n",
        "0.80 (Class 0), 0.88 (Class 1)\n",
        "\n",
        "Support: Number of samples in each class in the test set.\n",
        "\n",
        "Class 0: 742 instances\n",
        "\n",
        "Class 1: 1209 instances\n",
        "\n",
        "📈 Overall Model Performance:\n",
        "Accuracy: 85%\n",
        "\n",
        "85% of all test samples were correctly classified."
      ],
      "metadata": {
        "id": "6Ub8zdv8C6Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Visualizing evaluation Metric Score chart\n",
        "# 📘 Key Metrics Explained:\n",
        "# Precision: Out of all the predictions the model made for a class, how many were correct?\n",
        "\n",
        "# Class 0: 82% of negative predictions were actually negative.\n",
        "\n",
        "# Class 1: 88% of positive predictions were actually positive.\n",
        "\n",
        "# Recall: Out of all actual instances of a class, how many did the model correctly identify?\n",
        "\n",
        "# Class 0: 79% of true negatives were caught.\n",
        "\n",
        "# Class 1: 89% of true positives were caught.\n",
        "\n",
        "# F1-Score: Harmonic mean of precision and recall. A good balance indicator.\n",
        "\n",
        "# 0.80 (Class 0), 0.88 (Class 1)\n",
        "\n",
        "# Support: Number of samples in each class in the test set.\n",
        "\n",
        "# Class 0: 742 instances\n",
        "\n",
        "# Class 1: 1209 instances\n",
        "\n",
        "# 📈 Overall Model Performance:\n",
        "# Accuracy: 85%\n",
        "\n",
        "# 85% of all test samples were correctly classified. -->\n"
      ],
      "metadata": {
        "id": "-l8DnwduC1lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# Cross-validation and hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
        "grid_rf.fit(X_train_res, y_train_res)\n",
        "# Best model\n",
        "best_rf = grid_rf.best_estimator_\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "print(\"Optimized Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_best_rf))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is GridSearchCV?\n",
        "GridSearchCV is a method to systematically search through a range of hyperparameters for a machine learning model, in this case, a Random Forest. It performs cross-validation to find the best combination of parameters based on a scoring metric (here, accuracy).\n",
        "\n",
        "GridSearchCV tried every possible combination of these parameters using 3-fold cross-validation, meaning it trained and validated each setting on 3 different splits of the data.\n",
        "\n",
        "n_estimators=200: Using 200 decision trees improved performance.\n",
        "\n",
        "min_samples_split=5: A node must have at least 5 samples before it can be split. Helps prevent overfitting.\n",
        "\n",
        "random_state=42: Ensures reproducibility of results.\n",
        "\n",
        "Other parameters (like max_depth or min_samples_leaf) were likely found to not improve accuracy further in your case, so default values may have been retained.\n",
        "\n",
        "📈 Why this matters:\n",
        "Tuning improves model performance by finding the best parameter combo for your dataset.\n",
        "\n",
        "The final model (best_estimator_) should now be retrained on your full training set before testing."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# ========== ML Model 2: Logistic Regression ==========\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_res, y_train_res)\n",
        "y_pred_log = logreg_model.predict(X_test)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_log))\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Model Overview: Logistic Regression\n",
        "You implemented Logistic Regression (a linear model used for binary classification) to predict restaurant ratings (probably high vs low).\n",
        "\n",
        "Trained on: SMOTE-balanced training data (X_train_res, y_train_res)\n",
        "\n",
        "Tested on: Unseen test data (X_test, y_test)\n",
        "\n",
        "✅ Overall Performance Metrics\n",
        "Accuracy: 0.86 → 86% of all test predictions were correct.\n",
        "\n",
        "📈 Insights & Business Impact\n",
        "Why use Logistic Regression?\n",
        "\n",
        "It's simple, interpretable, and useful as a strong baseline model.\n",
        "\n",
        "Fast to train and works well when features have linear relationships with the outcome.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Logistic regression performs comparably well with an F1-score of 0.88 for high ratings and 0.82 for low ratings.\n",
        "\n",
        "This model shows balanced performance, useful for applications like flagging top restaurants or identifying underperformers.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Reliable predictions can guide:\n",
        "\n",
        "Customer targeting (e.g., promoting highly-rated restaurants)\n",
        "\n",
        "Restaurant improvement strategies (e.g., identify consistently low-rated ones)\n",
        "\n",
        "High recall for low ratings helps ensure negative experiences aren’t overlooked."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# ========== ML Model 3: Support Vector Classifier ==========\n",
        "svc_model = SVC()\n",
        "svc_model.fit(X_train_res, y_train_res)\n",
        "y_pred_svc = svc_model.predict(X_test)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svc))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        " - Random Forest performed well and improved with hyperparameter tuning using GridSearchCV.\n",
        " - Logistic Regression and SVC offer baseline comparisons.\n",
        " - Hyperparameter tuning improved F1-score and recall for the positive class, indicating better sentiment capture.\n"
      ],
      "metadata": {
        "id": "6KazkKcAtFVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- F1 Score: Balances Precision and Recall, important in imbalanced data.\n",
        "- Recall: Critical when identifying unhappy customers (negative sentiment).\n",
        "- Accuracy: Overall performance indicator.Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen Final Model: Optimized Random Forest\n",
        "- Best F1-score and recall for positive sentiment.\n",
        "- Robust to outliers and handles feature interactions well.Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train Random Forest\n",
        "best_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "best_rf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = best_rf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "aajIqGlLt656"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get importances for only numeric features\n",
        "importances = best_rf.feature_importances_[-X_scaled.shape[1]:]  # last N are numeric\n",
        "features = ['Cost', 'Num_Reviews', 'Num_Followers']\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(features, importances)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Feature Importance from Random Forest (Numeric Features)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DyGuiIqRtseU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 What It Indicates\n",
        "Cost and Num_Reviews are the most influential features, with nearly equal importance.\n",
        "\n",
        "Num_Followers has slightly less influence, but still contributes meaningfully.\n",
        "\n",
        "Random Forest calculates feature importance based on how much each feature reduces impurity (like Gini Index) across all decision trees.\n",
        "\n",
        "💡 Insights from the Chart\n",
        "Restaurants with higher costs and more reviews tend to have more predictable patterns in user ratings — possibly because higher-priced places are more polarizing or reviewed more frequently.\n",
        "\n",
        "Number of followers of a reviewer is less influential than the other two, but still plays a notable role in determining sentiment or trust level of a review.\n",
        "\n",
        "📈 Business Impact\n",
        "Targeting strategy: Focus on improving review volume and pricing strategy to enhance visibility and ratings.\n",
        "\n",
        "Trust scoring: Although reviewer popularity (followers) affects perception, it's not as critical as how much a place is reviewed or its pricing.\n",
        "\n",
        "This knowledge can help prioritize features in future models, simplify data collection, or focus business actions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 **Key Findings**:\n",
        "Cost and Number of Reviews are the most important numeric predictors of restaurant ratings.\n",
        "\n",
        "Text reviews required preprocessing (removal of noise, stopwords, stemming) to enable sentiment analysis and vectorization.\n",
        "\n",
        "Higher cost restaurants slightly correlate with better ratings, but the correlation is weak.\n",
        "\n",
        "Reviewers with more followers do not significantly influence rating scores.\n",
        "\n",
        "Logistic Regression and Random Forest achieved strong performance, with Random Forest performing slightly better.\n",
        "\n",
        "After applying SMOTE to handle class imbalance, model accuracy and F1-score improved.\n",
        "\n",
        "🤖 **Best Model Chosen**:\n",
        "Random Forest Classifier after hyperparameter tuning (via GridSearchCV), due to its higher accuracy and balanced performance across classes.\n",
        "\n",
        "📈 **Business Impact**:\n",
        "Improving review count and managing price points can positively affect restaurant ratings.\n",
        "\n",
        "Analysis allows Zomato to predict restaurant popularity, recommend top-rated places, and enhance customer satisfaction."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}